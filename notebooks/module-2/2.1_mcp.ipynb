{
 "cells": [
  {
   "cell_type": "code",
   "id": "717edf63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T08:48:28.867518Z",
     "start_time": "2026-02-20T08:48:28.841135Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "db64c0fd-9355-49e4-8290-2f2ae08eeef1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T08:48:30.073151Z",
     "start_time": "2026-02-20T08:48:30.064965Z"
    }
   },
   "source": [
    "import sys\n",
    "import asyncio\n",
    "\n",
    "# Fix for Windows issues in Jupyter notebooks\n",
    "if sys.platform == \"win32\":\n",
    "    # 1. Use ProactorEventLoop for subprocess support\n",
    "    if not isinstance(asyncio.get_event_loop_policy(), asyncio.WindowsProactorEventLoopPolicy):\n",
    "        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "    \n",
    "    # 2. Redirect stderr to avoid fileno() error when launching MCP servers\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        sys.stderr = sys.__stderr__\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "8d701224",
   "metadata": {},
   "source": [
    "## Local MCP server"
   ]
  },
  {
   "cell_type": "code",
   "id": "f11678d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T08:48:31.789693Z",
     "start_time": "2026-02-20T08:48:31.781140Z"
    }
   },
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"local_server\": {\n",
    "                \"transport\": \"stdio\",\n",
    "                \"command\": \"python\",\n",
    "                \"args\": [\"resources/2.1_mcp_server.py\"],\n",
    "            }\n",
    "    }\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "184db1d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T08:48:37.581764Z",
     "start_time": "2026-02-20T08:48:33.538549Z"
    }
   },
   "source": [
    "# get tools\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# get resources\n",
    "resources = await client.get_resources(\"local_server\")\n",
    "\n",
    "# get prompts\n",
    "prompt = await client.get_prompt(\"local_server\", \"prompt\")\n",
    "prompt = prompt[0].content"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "d548fad5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T08:48:37.605906Z",
     "start_time": "2026-02-20T08:48:37.582602Z"
    }
   },
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "model = ChatOllama(model=\"lfm2.5-thinking\")\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=tools,\n",
    "    system_prompt=prompt\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "5256ae3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T08:48:48.068480Z",
     "start_time": "2026-02-20T08:48:39.526907Z"
    }
   },
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me about the langchain-mcp-adapters library\")]},\n",
    "    config=config\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "3efb5bbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T08:46:04.798080Z",
     "start_time": "2026-02-20T08:46:04.734779Z"
    }
   },
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Tell me about the langchain-mcp-adapters library', additional_kwargs={}, response_metadata={}, id='66b647d2-dd94-47c5-a0d1-5ca710d125ff'),\n",
      "              AIMessage(content=\"The **langchain-mcp-adapters** library is part of the LangChain ecosystem designed to enhance model composition and integration capabilities. It allows users to adapt and combine models (e.g., from different frameworks like TensorFlow, PyTorch, or custom models) into cohesive workflows within LangChain. This facilitates seamless integration of diverse models into a unified pipeline, enabling tasks like model selection, training, or deployment within a single workflow. It supports modularity and flexibility, making it useful for customizing or extending LangChain's functionality. For specific implementation details, consult the LangChain documentation or community resources. Let me know if you'd like further clarification!\", additional_kwargs={}, response_metadata={'model': 'lfm2.5-thinking', 'created_at': '2026-02-20T08:46:02.611969Z', 'done': True, 'done_reason': 'stop', 'total_duration': 6012513834, 'load_duration': 481332584, 'prompt_eval_count': 220, 'prompt_eval_duration': 447026583, 'eval_count': 670, 'eval_duration': 4989276235, 'logprobs': None, 'model_name': 'lfm2.5-thinking', 'model_provider': 'ollama'}, id='lc_run--019c7a3a-23f4-7d62-88a3-c42242ce68d7-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 220, 'output_tokens': 670, 'total_tokens': 890})]}\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "847409a3",
   "metadata": {},
   "source": [
    "## Online MCP"
   ]
  },
  {
   "cell_type": "code",
   "id": "4b2895fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T08:50:40.667680Z",
     "start_time": "2026-02-20T08:50:40.637205Z"
    }
   },
   "source": [
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"time\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"ls\",\n",
    "            \"args\": [\n",
    "                \"mcp-server-time\",\n",
    "                \"--local-timezone=America/New_York\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"/Users/tomaszgaluszka/.pyenv/versions/lnd_env/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "  |     await eval(code_obj, self.user_global_ns, self.user_ns)\n",
      "  |   File \"/var/folders/76/l6f865dn69ddcxry4_d4l6br0000gn/T/ipykernel_43768/1663329054.py\", line 14, in <module>\n",
      "  |     tools = await client.get_tools()\n",
      "  |             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/Users/tomaszgaluszka/.pyenv/versions/lnd_env/lib/python3.13/site-packages/langchain_mcp_adapters/client.py\", line 197, in get_tools\n",
      "  |     tools_list = await asyncio.gather(*load_mcp_tool_tasks)\n",
      "  |                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/Users/tomaszgaluszka/.pyenv/versions/lnd_env/lib/python3.13/site-packages/langchain_mcp_adapters/tools.py\", line 478, in load_mcp_tools\n",
      "  |     async with create_session(\n",
      "  |                ~~~~~~~~~~~~~~^\n",
      "  |         connection, mcp_callbacks=mcp_callbacks\n",
      "  |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |     ) as tool_session:\n",
      "  |     ^\n",
      "  |   File \"/Users/tomaszgaluszka/.pyenv/versions/3.13.12/lib/python3.13/contextlib.py\", line 235, in __aexit__\n",
      "  |     await self.gen.athrow(value)\n",
      "  |   File \"/Users/tomaszgaluszka/.pyenv/versions/lnd_env/lib/python3.13/site-packages/langchain_mcp_adapters/sessions.py\", line 419, in create_session\n",
      "  |     async with _create_stdio_session(**params) as session:\n",
      "  |                ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "  |   File \"/Users/tomaszgaluszka/.pyenv/versions/3.13.12/lib/python3.13/contextlib.py\", line 235, in __aexit__\n",
      "  |     await self.gen.athrow(value)\n",
      "  |   File \"/Users/tomaszgaluszka/.pyenv/versions/lnd_env/lib/python3.13/site-packages/langchain_mcp_adapters/sessions.py\", line 231, in _create_stdio_session\n",
      "  |     stdio_client(server_params) as (read, write),\n",
      "  |     ~~~~~~~~~~~~^^^^^^^^^^^^^^^\n",
      "  |   File \"/Users/tomaszgaluszka/.pyenv/versions/3.13.12/lib/python3.13/contextlib.py\", line 235, in __aexit__\n",
      "  |     await self.gen.athrow(value)\n",
      "  |   File \"/Users/tomaszgaluszka/.pyenv/versions/lnd_env/lib/python3.13/site-packages/mcp/client/stdio/__init__.py\", line 183, in stdio_client\n",
      "  |     anyio.create_task_group() as tg,\n",
      "  |     ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  |   File \"/Users/tomaszgaluszka/.pyenv/versions/lnd_env/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 783, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  |         \"unhandled errors in a TaskGroup\", self._exceptions\n",
      "  |     ) from None\n",
      "  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Exception Group Traceback (most recent call last):\n",
      "    |   File \"/Users/tomaszgaluszka/.pyenv/versions/lnd_env/lib/python3.13/site-packages/mcp/client/stdio/__init__.py\", line 189, in stdio_client\n",
      "    |     yield read_stream, write_stream\n",
      "    |   File \"/Users/tomaszgaluszka/.pyenv/versions/lnd_env/lib/python3.13/site-packages/langchain_mcp_adapters/sessions.py\", line 232, in _create_stdio_session\n",
      "    |     ClientSession(read, write, **(session_kwargs or {})) as session,\n",
      "    |     ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/tomaszgaluszka/.pyenv/versions/lnd_env/lib/python3.13/site-packages/mcp/shared/session.py\", line 238, in __aexit__\n",
      "    |     return await self._task_group.__aexit__(exc_type, exc_val, exc_tb)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/tomaszgaluszka/.pyenv/versions/lnd_env/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 783, in __aexit__\n",
      "    |     raise BaseExceptionGroup(\n",
      "    |         \"unhandled errors in a TaskGroup\", self._exceptions\n",
      "    |     ) from None\n",
      "    | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "    +-+---------------- 1 ----------------\n",
      "      | Traceback (most recent call last):\n",
      "      |   File \"/Users/tomaszgaluszka/.pyenv/versions/lnd_env/lib/python3.13/site-packages/langchain_mcp_adapters/sessions.py\", line 234, in _create_stdio_session\n",
      "      |     yield session\n",
      "      |   File \"/Users/tomaszgaluszka/.pyenv/versions/lnd_env/lib/python3.13/site-packages/langchain_mcp_adapters/sessions.py\", line 420, in create_session\n",
      "      |     yield session\n",
      "      |   File \"/Users/tomaszgaluszka/.pyenv/versions/lnd_env/lib/python3.13/site-packages/langchain_mcp_adapters/tools.py\", line 481, in load_mcp_tools\n",
      "      |     await tool_session.initialize()\n",
      "      |   File \"/Users/tomaszgaluszka/.pyenv/versions/lnd_env/lib/python3.13/site-packages/mcp/client/session.py\", line 171, in initialize\n",
      "      |     result = await self.send_request(\n",
      "      |              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "      |     ...<16 lines>...\n",
      "      |     )\n",
      "      |     ^\n",
      "      |   File \"/Users/tomaszgaluszka/.pyenv/versions/lnd_env/lib/python3.13/site-packages/mcp/shared/session.py\", line 306, in send_request\n",
      "      |     raise McpError(response_or_error.error)\n",
      "      | mcp.shared.exceptions.McpError: Connection closed\n",
      "      +------------------------------------\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "e264dd69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T08:47:21.669595Z",
     "start_time": "2026-02-20T08:47:21.646738Z"
    }
   },
   "source": [
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=tools,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "4725cee3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T08:47:26.997651Z",
     "start_time": "2026-02-20T08:47:24.256881Z"
    }
   },
   "source": [
    "question = HumanMessage(content=\"What time is it?\")\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "pprint(response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What time is it?', additional_kwargs={}, response_metadata={}, id='ca9d0769-57c2-4512-aac3-49ac3929abb1'),\n",
      "              AIMessage(content='I don\\'t have real-time access to current time information. However, you can check the current time using:\\n\\n1. Your device\\'s clock/app  \\n2. A search engine (e.g., \"What time is it today?\")  \\n3. A world clock website  \\n\\nIf you\\'d like, I can help you find a reliable source for this information! Just let me know your location or preferred time zone.', additional_kwargs={}, response_metadata={'model': 'lfm2.5-thinking', 'created_at': '2026-02-20T08:47:26.977863Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2711434042, 'load_duration': 39688125, 'prompt_eval_count': 72, 'prompt_eval_duration': 99272000, 'eval_count': 343, 'eval_duration': 2523821498, 'logprobs': None, 'model_name': 'lfm2.5-thinking', 'model_provider': 'ollama'}, id='lc_run--019c7a3b-7a67-7233-b607-5ef2c789a3e9-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 72, 'output_tokens': 343, 'total_tokens': 415})]}\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bc5152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
